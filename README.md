## [Wikiextractor](https://github.com/attardi/wikiextractor) library is working fine. Scraping this project.

Check out [this article](https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67) to extract wiki dump.
<!-- 
<h2 align="center">Wikipedia corpus</h2>
<p align="center">
<a href="https://github.com/psf/black"><img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
</p>

# Odia wikipedia monolingual corpus extraction
Different types of Odia language data with their sources.  
Currently the Odia wikipedia monolingual corpus collection script is in progress.  This script once completed will be used as a template to collect corpus from many other Odia sources.
**Warning**: `The code is on an intermediate state and not working.`

#### Command to run the script
Go inside the data folder and run the following command:
```
 python ../scripts/wikipedia/wikipedia_article_extractor.py 
```

## Acknowledgements
- [Gaurav Arora](https://github.com/goru001/nlp-for-odia) for Wikipedia scraping algorithm
- [Adnan Siddiqi](https://towardsdatascience.com/5-strategies-to-write-unblock-able-web-scrapers-in-python-5e40c147bdaf) for headers of data scraping
-->
